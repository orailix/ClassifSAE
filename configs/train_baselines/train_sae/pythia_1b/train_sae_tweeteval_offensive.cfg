[main]
model=pythia-1b
model_path=EleutherAI/pythia-1b
dataset=tweeteval_offensive
task=sae_training

[version]
finetuned=True
latest_version = False
checkpoint_version=30000
dataset_tuned_on=tweeteval_offensive

[task_args]
activation_fn=topk
topk=10
training_tokens=10000000
len_epoch=11916
d_in=2048
d_sae=4096
hook_layer=15
expansion_factor=0
store_batch_size_prompts=4
n_batches_in_buffer=1500
save_label=True
prompt_tuning=False
train_batch_size_tokens=500
wandb_log_frequency=1
seed=42
use_ghost_grads=True
decoder_orthogonal_init=False
init_encoder_as_decoder_transpose=True
b_dec_init_method=zeros
l1_coefficient=0.
mse_loss_normalization=dense_batch
normalize_sae_decoder=True
lmbda_mse=1
lmbda_activation_rate=0.
lmbda_vcr=0
lmbda_decoder_columns_similarity=0.
lmbda_classifier=0
num_classifier_features=20
nb_classes=2
feature_activation_rate=[1.]
lr=5e-5
lr_scheduler_name=cosineannealing
lr_end=5e-7
feature_sampling_window=100
dead_feature_window=100
dead_feature_threshold=1e-4