[main]
model=pythia-70m
model_path=EleutherAI/pythia-70m
dataset=imdb
task=sae_training

[version]
finetuned=True
latest_version = False
checkpoint_version=30000
dataset_tuned_on=imdb

[task_args]
activation_fn=topk
topk=10
training_tokens=10000000
len_epoch=25000
d_in=512
d_sae=1024
hook_layer=5
expansion_factor=0
store_batch_size_prompts=4
n_batches_in_buffer=1250
eos=False
save_label=True
prompt_tuning=False
train_batch_size_tokens=500
wandb_log_frequency=1
seed=42
use_ghost_grads=True
decoder_orthogonal_init=False
init_encoder_as_decoder_transpose=True
b_dec_init_method=zeros
l1_coefficient=0.
mse_loss_normalization=dense_batch
normalize_sae_decoder=True
lmbda_mse=1
lmbda_feature_sparsity=0.
lmbda_vcr=0
lmbda_decoder_columns_similarity=0.
lmbda_classifier=0
num_classifier_features=20
nb_classes=2
feature_activation_rate=[1.]
lr=5e-5
lr_scheduler_name=cosineannealing
lr_end=5e-7
feature_sampling_window=100
dead_feature_window=100
dead_feature_threshold=1e-4