[main]
model=llama_8b_instruct
model_path=meta-llama/Llama-3.1-8B-Instruct
dataset=tweeteval_offensive
task=caching
split=train

[version]
finetuned=False

[prompt_tuning]
vtok=10
total_steps=2000

[task_args]
training_tokens=11916
store_batch_size_prompts=4
n_batches_in_buffer=1500
d_in=4096
eos=False
hook_layer=31
prompt_tuning=False
save_label=True