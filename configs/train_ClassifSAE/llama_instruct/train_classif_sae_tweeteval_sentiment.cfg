[main]
model=llama_8b_instruct
model_path=meta-llama/Llama-3.1-8B-Instruct
dataset=tweeteval_sentiment
task=sae_training

[version]
finetuned=False

[task_args]
activation_fn=topk
topk=10
training_tokens=10000000
len_epoch=45615
d_in=4096
d_sae=8192
hook_layer=31
expansion_factor=0
store_batch_size_prompts=4
n_batches_in_buffer=1500
save_label=True
eos=False
prompt_tuning=False
train_batch_size_tokens=1000
wandb_log_frequency=1
seed=42
use_ghost_grads=True
decoder_orthogonal_init=False
init_encoder_as_decoder_transpose=True
b_dec_init_method=zeros
l1_coefficient=0
mse_loss_normalization=dense_batch
normalize_sae_decoder=True
lmbda_mse=0.01
lmbda_activation_rate=0.01
lmbda_vcr=0
lmbda_decoder_columns_similarity=0.0
lmbda_classifier=1
num_classifier_features=20
nb_classes=3
feature_activation_rate=[0.1]
lr=5e-5
lr_scheduler_name=cosineannealing
lr_end=5e-7
feature_sampling_window=100
dead_feature_window=100
dead_feature_threshold=1e-4