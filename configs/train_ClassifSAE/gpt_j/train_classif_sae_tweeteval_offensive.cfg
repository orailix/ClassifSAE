[main]
model=gpt-j-6B
model_path=EleutherAI/gpt-j-6B
dataset=tweeteval_offensive
task=sae_training

[version]
finetuned=True
latest_version = False
checkpoint_version=15000
dataset_tuned_on=tweeteval_offensive

[task_args]
activation_fn=topk
topk=10
training_tokens=10000000
len_epoch=11916
d_in=4096
d_sae=8192
hook_layer=27
expansion_factor=0
store_batch_size_prompts=4
n_batches_in_buffer=1500
save_label=True
prompt_tuning=False
eos=False
train_batch_size_tokens=2000
wandb_log_frequency=1
seed=42
use_ghost_grads=True
decoder_orthogonal_init=False
init_encoder_as_decoder_transpose=True
b_dec_init_method=zeros
l1_coefficient=0
mse_loss_normalization=dense_batch
normalize_sae_decoder=True
lmbda_mse=0.01
lmbda_activation_rate=0.01
lmbda_vcr=0
lmbda_decoder_columns_similarity=0.0
lmbda_classifier=1
num_classifier_features=20
nb_classes=2
feature_activation_rate=[0.1]
lr=5e-5
lr_scheduler_name=cosineannealing
lr_end=5e-7
feature_sampling_window=100
dead_feature_window=100
dead_feature_threshold=1e-4