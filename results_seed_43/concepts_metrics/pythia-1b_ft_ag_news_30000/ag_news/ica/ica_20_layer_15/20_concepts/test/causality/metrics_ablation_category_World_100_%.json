{
    "category": "World",
    "Percentage ablation of the segment": 100,
    "Ablated features": [
        8,
        2,
        5,
        4
    ],
    "Global Absolute Accuracy change": -0.10355263948440552,
    "Global Total Variation Distance with regard to original predictions": 0.13279695808887482,
    "Global label-flip rate": 0.12513157725334167,
    "number real samples_0": 1900,
    "true matches_0": 1191,
    "number predicted samples_0": 1346,
    "recall_0": 0.6268421052631579,
    "precision_0": 0.8848439821693908,
    "f1-score_0": 0.7338262476894639,
    "number real samples_1": 1900,
    "true matches_1": 1871,
    "number predicted samples_1": 1902,
    "recall_1": 0.9847368421052631,
    "precision_1": 0.9837013669821241,
    "f1-score_1": 0.9842188321935822,
    "number real samples_2": 1900,
    "true matches_2": 1690,
    "number predicted samples_2": 1899,
    "recall_2": 0.8894736842105263,
    "precision_2": 0.889942074776198,
    "f1-score_2": 0.8897078178468019,
    "number real samples_3": 1900,
    "true matches_3": 1606,
    "number predicted samples_3": 2453,
    "recall_3": 0.8452631578947368,
    "precision_3": 0.6547085201793722,
    "f1-score_3": 0.7378819205145876
}